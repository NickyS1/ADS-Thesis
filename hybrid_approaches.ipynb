{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid approaches"
      ],
      "metadata": {
        "id": "rQOGqWaIeooZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes and Pattern.nl"
      ],
      "metadata": {
        "id": "IO9AMhMUhran"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install pattern\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "RaKw1O-FesxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68e2e5e-84d0-4a88-ae2c-6a6f6343d90a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.6.0-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.4)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=0f848f3c5f9a8eee6cc1c781479521aa5cedc5d6531ae7c0961ed90a6acf539c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124737 sha256=f99e9dce5bfb7b54e2bc0ca8f5328040c377e628c5342a611289a8edffab3adf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=d63b971948590a41407e1b74ce40f7b6bd6d2dab6aca61652a3744722c7750b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.1 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.6.0 zc.lockfile-3.0.post1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pattern.nl import sentiment\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MaxAbsScaler  # Changed MinMaxScaler to MaxAbsScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Setting the seed\n",
        "set_seed(42)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Custom transformer to preprocess Dutch text\n",
        "class DutchTextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer('dutch')\n",
        "        self.stop_words = set(stopwords.words('dutch'))\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        preprocessed_texts = []\n",
        "        for text in X:\n",
        "            # Convert to lowercase\n",
        "            text = text.lower()\n",
        "            # Tokenize text\n",
        "            tokens = word_tokenize(text, language='dutch')\n",
        "            # Remove punctuation and numbers\n",
        "            tokens = [word for word in tokens if word.isalnum()]\n",
        "            # Remove stop words\n",
        "            tokens = [word for word in tokens if word not in self.stop_words]\n",
        "            # Stemming\n",
        "            tokens = [self.stemmer.stem(word) for word in tokens]\n",
        "            # Join tokens back into string\n",
        "            preprocessed_text = ' '.join(tokens)\n",
        "            preprocessed_texts.append(preprocessed_text)\n",
        "        return preprocessed_texts\n",
        "\n",
        "# Function to perform sentiment analysis and generate classification report\n",
        "def cross_val_analysis(X_train_val, y_train_val, X_test, y_test):\n",
        "    tfidf = TfidfVectorizer()\n",
        "    scaler = MaxAbsScaler()  # Changed to MaxAbsScaler\n",
        "    preprocessor = DutchTextPreprocessor()\n",
        "\n",
        "    # Preprocess training and validation data\n",
        "    X_train_val_preprocessed = preprocessor.fit_transform(X_train_val)\n",
        "\n",
        "    # Fit and transform the training data with TF-IDF\n",
        "    X_train_val_tfidf = tfidf.fit_transform(X_train_val_preprocessed)\n",
        "\n",
        "    # Scale TF-IDF features\n",
        "    X_train_val_tfidf_scaled = scaler.fit_transform(X_train_val_tfidf)\n",
        "\n",
        "    # Transform the test data with preprocessor and TF-IDF\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "    X_test_tfidf = tfidf.transform(X_test_preprocessed)\n",
        "    X_test_tfidf_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "    # Apply SMOTE to the training data\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_train_val_tfidf_scaled, y_train_val)\n",
        "\n",
        "    nb_classifier = MultinomialNB()\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    y_pred_cv = cross_val_predict(nb_classifier, X_resampled, y_resampled, cv=skf)\n",
        "\n",
        "    # Cross-validation classification report and confusion matrix\n",
        "    report_cv = classification_report(y_resampled, y_pred_cv, zero_division=0)\n",
        "    cm_cv = confusion_matrix(y_resampled, y_pred_cv, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "    # Train final model on the entire training+validation set and test on the unseen test set\n",
        "    nb_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred_test = nb_classifier.predict(X_test_tfidf_scaled)\n",
        "\n",
        "    # Test set classification report and confusion matrix\n",
        "    report_test = classification_report(y_test, y_pred_test, zero_division=0)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "    return report_cv, cm_cv, report_test, cm_test\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]  # use the name from the CSV files\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Split the dataset into 85% training+validation and 15% test sets\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "    # Further split the training+validation set so that in the end there is 70% training and 15% validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
        "\n",
        "    # Combine training and validation sets for cross-validation\n",
        "    X_train_val_combined = pd.concat([X_train, X_val])\n",
        "    y_train_val_combined = pd.concat([y_train, y_val])\n",
        "\n",
        "    # Perform cross-validation analysis\n",
        "    report_cv, cm_cv, report_test, cm_test = cross_val_analysis(X_train_val_combined, y_train_val_combined, X_test, y_test)\n",
        "\n",
        "    # Print cross-validation classification report and confusion matrix\n",
        "    print(f\"Cross-Validation Classification Report for {dataset_name}:\\n\", report_cv)\n",
        "    print(f\"Cross-Validation Confusion Matrix for {dataset_name}:\\n\", cm_cv)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Print test set classification report and confusion matrix\n",
        "    print(f\"Test Set Classification Report for {dataset_name}:\\n\", report_test)\n",
        "    print(f\"Test Set Confusion Matrix for {dataset_name}:\\n\", cm_test)\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS8wy0bmpMnY",
        "outputId": "91f9c15a-52c4-4342-d593-f75911ae96ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n",
            "Cross-Validation Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.89      0.89      0.89       187\n",
            "     neutral       0.76      0.76      0.76       187\n",
            "    positive       0.75      0.74      0.75       187\n",
            "\n",
            "    accuracy                           0.80       561\n",
            "   macro avg       0.80      0.80      0.80       561\n",
            "weighted avg       0.80      0.80      0.80       561\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1960s_gas:\n",
            " [[167   9  11]\n",
            " [  9 143  35]\n",
            " [ 12  36 139]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      0.33      0.36        12\n",
            "     neutral       0.36      0.25      0.29        20\n",
            "    positive       0.54      0.67      0.59        33\n",
            "\n",
            "    accuracy                           0.48        65\n",
            "   macro avg       0.43      0.42      0.42        65\n",
            "weighted avg       0.46      0.48      0.46        65\n",
            "\n",
            "Test Set Confusion Matrix for 1960s_gas:\n",
            " [[ 4  3  5]\n",
            " [ 1  5 14]\n",
            " [ 5  6 22]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n",
            "Cross-Validation Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.66      0.96      0.78        46\n",
            "     neutral       0.89      0.91      0.90        46\n",
            "    positive       0.83      0.43      0.57        46\n",
            "\n",
            "    accuracy                           0.77       138\n",
            "   macro avg       0.79      0.77      0.75       138\n",
            "weighted avg       0.79      0.77      0.75       138\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1970s_gas:\n",
            " [[44  0  2]\n",
            " [ 2 42  2]\n",
            " [21  5 20]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.67      0.67      0.67         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.67      0.89      0.76         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.44      0.52      0.48        15\n",
            "weighted avg       0.53      0.67      0.59        15\n",
            "\n",
            "Test Set Confusion Matrix for 1970s_gas:\n",
            " [[2 0 1]\n",
            " [0 0 3]\n",
            " [1 0 8]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n",
            "Cross-Validation Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.51      0.48      0.50        75\n",
            "     neutral       0.77      0.79      0.78        75\n",
            "    positive       0.51      0.53      0.52        75\n",
            "\n",
            "    accuracy                           0.60       225\n",
            "   macro avg       0.60      0.60      0.60       225\n",
            "weighted avg       0.60      0.60      0.60       225\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1980s_gas:\n",
            " [[36  8 31]\n",
            " [ 9 59  7]\n",
            " [25 10 40]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.62      0.55        13\n",
            "     neutral       0.43      0.43      0.43         7\n",
            "    positive       0.40      0.31      0.35        13\n",
            "\n",
            "    accuracy                           0.45        33\n",
            "   macro avg       0.44      0.45      0.44        33\n",
            "weighted avg       0.45      0.45      0.45        33\n",
            "\n",
            "Test Set Confusion Matrix for 1980s_gas:\n",
            " [[8 2 3]\n",
            " [1 3 3]\n",
            " [7 2 4]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n",
            "Cross-Validation Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.71      0.48      0.57        21\n",
            "     neutral       0.69      0.95      0.80        21\n",
            "    positive       0.85      0.81      0.83        21\n",
            "\n",
            "    accuracy                           0.75        63\n",
            "   macro avg       0.75      0.75      0.73        63\n",
            "weighted avg       0.75      0.75      0.73        63\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1990s_gas:\n",
            " [[10  8  3]\n",
            " [ 1 20  0]\n",
            " [ 3  1 17]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00         4\n",
            "     neutral       0.00      0.00      0.00         2\n",
            "    positive       0.17      0.50      0.25         2\n",
            "\n",
            "    accuracy                           0.12         8\n",
            "   macro avg       0.06      0.17      0.08         8\n",
            "weighted avg       0.04      0.12      0.06         8\n",
            "\n",
            "Test Set Confusion Matrix for 1990s_gas:\n",
            " [[0 0 4]\n",
            " [1 0 1]\n",
            " [1 0 1]]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine and Pattern.nl"
      ],
      "metadata": {
        "id": "I5-9BKLh_nwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "IUlMrMeDABsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc76e628-8c36-4a22-fdd7-52cf62d7230d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pattern in /usr/local/lib/python3.10/dist-packages (3.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.10/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.10/dist-packages (from pattern) (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from pattern) (6.0.11)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from pattern) (20231228)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from pattern) (1.1.2)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.10/dist-packages (from pattern) (18.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.0.1)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (3.2.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (3.0.post1)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (5.0.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.10/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (4.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.10/dist-packages (from portend>=2.1.1->cherrypy->pattern) (5.6.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.10/dist-packages (from jaraco.collections->cherrypy->pattern) (3.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.10/dist-packages (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern) (1.2.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pattern.nl import sentiment\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to set all seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Setting the seed\n",
        "set_seed(42)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Custom transformer to preprocess Dutch text\n",
        "class DutchTextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer('dutch')\n",
        "        self.stop_words = set(stopwords.words('dutch'))\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        preprocessed_texts = []\n",
        "        for text in X:\n",
        "            # Convert to lowercase\n",
        "            text = text.lower()\n",
        "            # Tokenize text\n",
        "            tokens = word_tokenize(text, language='dutch')\n",
        "            # Remove punctuation and numbers\n",
        "            tokens = [word for word in tokens if word.isalnum()]\n",
        "            # Remove stop words\n",
        "            tokens = [word for word in tokens if word not in self.stop_words]\n",
        "            # Stemming\n",
        "            tokens = [self.stemmer.stem(word) for word in tokens]\n",
        "            # Join tokens back into string\n",
        "            preprocessed_text = ' '.join(tokens)\n",
        "            preprocessed_texts.append(preprocessed_text)\n",
        "        return preprocessed_texts\n",
        "\n",
        "# Custom transformer for lexicon-based features\n",
        "class LexiconBasedTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass  # Add any initialization if needed\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        lexicon_features = []\n",
        "        for text in X:\n",
        "            polarity, _ = sentiment(text)\n",
        "            lexicon_features.append([polarity])\n",
        "        return np.array(lexicon_features)\n",
        "\n",
        "# Function to perform sentiment analysis and generate classification report\n",
        "def cross_val_analysis(X_train_val, y_train_val, X_test, y_test):\n",
        "    tfidf = TfidfVectorizer()\n",
        "    lexicon_transformer = LexiconBasedTransformer()\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Preprocess training and validation data\n",
        "    preprocessor = DutchTextPreprocessor()\n",
        "    X_train_val_preprocessed = preprocessor.fit_transform(X_train_val)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Fit and transform the training data with TF-IDF and lexicon-based transformer\n",
        "    X_train_val_tfidf = tfidf.fit_transform(X_train_val_preprocessed)\n",
        "    X_train_val_lexicon = lexicon_transformer.fit_transform(X_train_val_preprocessed)\n",
        "    X_train_val_lexicon = scaler.fit_transform(X_train_val_lexicon)  # Scale lexicon features\n",
        "    X_train_val_combined = hstack([X_train_val_tfidf, X_train_val_lexicon])\n",
        "\n",
        "    # Transform the test data with TF-IDF and lexicon-based transformer\n",
        "    X_test_tfidf = tfidf.transform(X_test_preprocessed)\n",
        "    X_test_lexicon = lexicon_transformer.transform(X_test_preprocessed)\n",
        "    X_test_lexicon = scaler.transform(X_test_lexicon)  # Scale lexicon features\n",
        "    X_test_combined = hstack([X_test_tfidf, X_test_lexicon])\n",
        "\n",
        "    # Apply SMOTE to the training data\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_train_val_combined, y_train_val)\n",
        "\n",
        "    svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    y_pred_cv = cross_val_predict(svm_classifier, X_resampled, y_resampled, cv=skf)\n",
        "\n",
        "    # Cross-validation classification report and confusion matrix\n",
        "    report_cv = classification_report(y_resampled, y_pred_cv, zero_division=0)\n",
        "    cm_cv = confusion_matrix(y_resampled, y_pred_cv, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "    # Train final model on the entire training+validation set and test on the unseen test set\n",
        "    svm_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred_test = svm_classifier.predict(X_test_combined)\n",
        "\n",
        "    # Test set classification report and confusion matrix\n",
        "    report_test = classification_report(y_test, y_pred_test, zero_division=0)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "    return report_cv, cm_cv, report_test, cm_test\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Split the dataset into 85% training+validation and 15% test sets\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "    # Further split the training+validation set so that in the end there is 70% training and 15% validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val)\n",
        "\n",
        "    # Combine training and validation sets for cross-validation\n",
        "    X_train_val_combined = pd.concat([X_train, X_val])\n",
        "    y_train_val_combined = pd.concat([y_train, y_val])\n",
        "\n",
        "    # Perform cross-validation analysis\n",
        "    report_cv, cm_cv, report_test, cm_test = cross_val_analysis(X_train_val_combined, y_train_val_combined, X_test, y_test)\n",
        "\n",
        "    # Print cross-validation classification report and confusion matrix\n",
        "    print(f\"Cross-Validation Classification Report for {dataset_name}:\\n\", report_cv)\n",
        "    print(f\"Cross-Validation Confusion Matrix for {dataset_name}:\\n\", cm_cv)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Print test set classification report and confusion matrix\n",
        "    print(f\"Test Set Classification Report for {dataset_name}:\\n\", report_test)\n",
        "    print(f\"Test Set Confusion Matrix for {dataset_name}:\\n\", cm_test)\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo6K8pAkqJ4I",
        "outputId": "efbde5e9-0bac-4411-d668-73f5ef2cf77d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n",
            "Cross-Validation Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.94      0.92       187\n",
            "     neutral       0.80      0.79      0.80       187\n",
            "    positive       0.78      0.75      0.77       187\n",
            "\n",
            "    accuracy                           0.83       561\n",
            "   macro avg       0.83      0.83      0.83       561\n",
            "weighted avg       0.83      0.83      0.83       561\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1960s_gas:\n",
            " [[175   2  10]\n",
            " [  9 148  30]\n",
            " [ 11  35 141]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.25      0.29        12\n",
            "     neutral       0.36      0.25      0.29        20\n",
            "    positive       0.55      0.70      0.61        33\n",
            "\n",
            "    accuracy                           0.48        65\n",
            "   macro avg       0.41      0.40      0.40        65\n",
            "weighted avg       0.45      0.48      0.45        65\n",
            "\n",
            "Test Set Confusion Matrix for 1960s_gas:\n",
            " [[ 3  2  7]\n",
            " [ 3  5 12]\n",
            " [ 3  7 23]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n",
            "Cross-Validation Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.98      0.93      0.96        46\n",
            "     neutral       0.95      0.85      0.90        46\n",
            "    positive       0.81      0.93      0.87        46\n",
            "\n",
            "    accuracy                           0.91       138\n",
            "   macro avg       0.91      0.91      0.91       138\n",
            "weighted avg       0.91      0.91      0.91       138\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1970s_gas:\n",
            " [[43  0  3]\n",
            " [ 0 39  7]\n",
            " [ 1  2 43]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.33      0.50         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.64      1.00      0.78         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.55      0.44      0.43        15\n",
            "weighted avg       0.59      0.67      0.57        15\n",
            "\n",
            "Test Set Confusion Matrix for 1970s_gas:\n",
            " [[1 0 2]\n",
            " [0 0 3]\n",
            " [0 0 9]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n",
            "Cross-Validation Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.54      0.57      0.55        75\n",
            "     neutral       0.89      0.77      0.83        75\n",
            "    positive       0.51      0.55      0.53        75\n",
            "\n",
            "    accuracy                           0.63       225\n",
            "   macro avg       0.65      0.63      0.64       225\n",
            "weighted avg       0.65      0.63      0.64       225\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1980s_gas:\n",
            " [[43  3 29]\n",
            " [ 7 58 10]\n",
            " [30  4 41]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.62      0.38      0.48        13\n",
            "     neutral       0.00      0.00      0.00         7\n",
            "    positive       0.35      0.62      0.44        13\n",
            "\n",
            "    accuracy                           0.39        33\n",
            "   macro avg       0.32      0.33      0.31        33\n",
            "weighted avg       0.38      0.39      0.36        33\n",
            "\n",
            "Test Set Confusion Matrix for 1980s_gas:\n",
            " [[5 0 8]\n",
            " [0 0 7]\n",
            " [3 2 8]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n",
            "Cross-Validation Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.81      0.72        21\n",
            "     neutral       0.86      0.86      0.86        21\n",
            "    positive       0.94      0.71      0.81        21\n",
            "\n",
            "    accuracy                           0.79        63\n",
            "   macro avg       0.82      0.79      0.80        63\n",
            "weighted avg       0.82      0.79      0.80        63\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1990s_gas:\n",
            " [[17  3  1]\n",
            " [ 3 18  0]\n",
            " [ 6  0 15]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      0.50      0.44         4\n",
            "     neutral       0.00      0.00      0.00         2\n",
            "    positive       0.33      0.50      0.40         2\n",
            "\n",
            "    accuracy                           0.38         8\n",
            "   macro avg       0.24      0.33      0.28         8\n",
            "weighted avg       0.28      0.38      0.32         8\n",
            "\n",
            "Test Set Confusion Matrix for 1990s_gas:\n",
            " [[2 0 2]\n",
            " [2 0 0]\n",
            " [1 0 1]]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM, Pattern.nl and LUPJE"
      ],
      "metadata": {
        "id": "o34F-IdN3n5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install imbalanced-learn\n",
        "!pip install pattern\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "uzLEQWcYarVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7adf71bd-7edd-4c5d-fc5e-d6151efba662"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: pattern in /usr/local/lib/python3.10/dist-packages (3.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.10/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.10/dist-packages (from pattern) (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from pattern) (6.0.11)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from pattern) (20231228)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from pattern) (1.1.2)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.10/dist-packages (from pattern) (18.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.0.1)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (3.2.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (3.0.post1)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (5.0.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.10/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (4.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.10/dist-packages (from portend>=2.1.1->cherrypy->pattern) (5.6.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.10/dist-packages (from jaraco.collections->cherrypy->pattern) (3.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.10/dist-packages (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern) (1.2.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import hstack\n",
        "from pattern.nl import sentiment\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to set all seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Load the second lexicon\n",
        "words_sentiment_df = pd.DataFrame(columns=[\"word\", \"sentiment_score\"])\n",
        "\n",
        "with open(\"LUPJE.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        try:\n",
        "            word, sentiment_score = line.strip().split(\"\\t\")\n",
        "            words_sentiment_df = pd.concat([words_sentiment_df, pd.DataFrame({\"word\": [word], \"sentiment_score\": [sentiment_score]})])\n",
        "        except ValueError:\n",
        "            print(f\"Skipping line with incorrect formatting: {line.strip()}\")\n",
        "\n",
        "# Convert sentiment scores to numeric type\n",
        "words_sentiment_df[\"sentiment_score\"] = pd.to_numeric(words_sentiment_df[\"sentiment_score\"])\n",
        "\n",
        "# Custom transformer for text preprocessing\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('dutch'))\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        preprocessed_texts = [self.preprocess_text(text) for text in X]\n",
        "        return preprocessed_texts\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove punctuation and numbers\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text, language='dutch')\n",
        "        # Remove stop words\n",
        "        tokens = [word for word in tokens if word not in self.stop_words]\n",
        "        # Join tokens back to string\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Custom transformer to add lexicon-based features from both lexicons\n",
        "class CombinedLexiconTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        lexicon_features = []\n",
        "        for text in X:\n",
        "            # First lexicon (Pattern.nl sentiment)\n",
        "            polarity_pattern, _ = sentiment(text)\n",
        "\n",
        "            # Second lexicon (LUPJE)\n",
        "            tokens = text.split()\n",
        "            sentiment_score_lupje = 0\n",
        "            for token in tokens:\n",
        "                if token in words_sentiment_df[\"word\"].values:\n",
        "                    sentiment_score_lupje += words_sentiment_df.loc[words_sentiment_df[\"word\"] == token, \"sentiment_score\"].values[0]\n",
        "\n",
        "            lexicon_features.append([polarity_pattern, sentiment_score_lupje])\n",
        "        return np.array(lexicon_features)\n",
        "\n",
        "# Function to perform sentiment analysis and generate classification report\n",
        "def cross_val_analysis(X_train_val, y_train_val, X_test, y_test, seed):\n",
        "    text_preprocessor = TextPreprocessor()\n",
        "    tfidf = TfidfVectorizer()\n",
        "    lexicon_transformer = CombinedLexiconTransformer()\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Preprocess the text data\n",
        "    X_train_val_preprocessed = text_preprocessor.fit_transform(X_train_val)\n",
        "    X_test_preprocessed = text_preprocessor.transform(X_test)\n",
        "\n",
        "    # Fit and transform the training data with TF-IDF and lexicon-based transformer\n",
        "    X_train_val_tfidf = tfidf.fit_transform(X_train_val_preprocessed)\n",
        "    X_train_val_lexicon = lexicon_transformer.fit_transform(X_train_val_preprocessed)\n",
        "    X_train_val_lexicon = scaler.fit_transform(X_train_val_lexicon)  # Scale lexicon features\n",
        "    X_train_val_combined = hstack([X_train_val_tfidf, X_train_val_lexicon])\n",
        "\n",
        "    # Transform the test data with TF-IDF and lexicon-based transformer\n",
        "    X_test_tfidf = tfidf.transform(X_test_preprocessed)\n",
        "    X_test_lexicon = lexicon_transformer.transform(X_test_preprocessed)\n",
        "    X_test_lexicon = scaler.transform(X_test_lexicon)  # Scale lexicon features\n",
        "    X_test_combined = hstack([X_test_tfidf, X_test_lexicon])\n",
        "\n",
        "    # Apply SMOTE to the training data\n",
        "    smote = SMOTE(random_state=seed)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_train_val_combined, y_train_val)\n",
        "\n",
        "    svm_classifier = SVC(kernel='linear', random_state=seed)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    y_pred_cv = cross_val_predict(svm_classifier, X_resampled, y_resampled, cv=skf)\n",
        "\n",
        "    # Cross-validation classification report and confusion matrix\n",
        "    report_cv = classification_report(y_resampled, y_pred_cv, zero_division=0)\n",
        "    cm_cv = confusion_matrix(y_resampled, y_pred_cv, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "    # Train final model on the entire training+validation set and test on the unseen test set\n",
        "    svm_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred_test = svm_classifier.predict(X_test_combined)\n",
        "\n",
        "    # Test set classification report and confusion matrix\n",
        "    report_test = classification_report(y_test, y_pred_test, zero_division=0)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "    return report_cv, cm_cv, report_test, cm_test\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]  # use the name from the CSV files\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Split the dataset into 85% training+validation and 15% test sets\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=seed, stratify=y)\n",
        "\n",
        "    # Further split the training+validation set into 70% training and 15% validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=seed, stratify=y_train_val)\n",
        "\n",
        "    # Combine training and validation sets for cross-validation\n",
        "    X_train_val_combined = pd.concat([X_train, X_val])\n",
        "    y_train_val_combined = pd.concat([y_train, y_val])\n",
        "\n",
        "    # Perform cross-validation analysis\n",
        "    report_cv, cm_cv, report_test, cm_test = cross_val_analysis(X_train_val_combined, y_train_val_combined, X_test, y_test, seed)\n",
        "\n",
        "    # Print cross-validation classification report and confusion matrix\n",
        "    print(f\"Cross-Validation Classification Report for {dataset_name}:\\n\", report_cv)\n",
        "    print(f\"Cross-Validation Confusion Matrix for {dataset_name}:\\n\", cm_cv)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Print test set classification report and confusion matrix\n",
        "    print(f\"Test Set Classification Report for {dataset_name}:\\n\", report_test)\n",
        "    print(f\"Test Set Confusion Matrix for {dataset_name}:\\n\", cm_test)\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YjhogZS1FNw",
        "outputId": "5ac1f304-0ca8-4cfb-c313-b38ad140b527"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n",
            "Cross-Validation Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.95      0.93       187\n",
            "     neutral       0.78      0.79      0.79       187\n",
            "    positive       0.78      0.73      0.76       187\n",
            "\n",
            "    accuracy                           0.83       561\n",
            "   macro avg       0.82      0.83      0.82       561\n",
            "weighted avg       0.82      0.83      0.82       561\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1960s_gas:\n",
            " [[178   4   5]\n",
            " [  6 148  33]\n",
            " [ 12  38 137]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.25      0.29        12\n",
            "     neutral       0.27      0.15      0.19        20\n",
            "    positive       0.53      0.73      0.62        33\n",
            "\n",
            "    accuracy                           0.46        65\n",
            "   macro avg       0.38      0.38      0.36        65\n",
            "weighted avg       0.42      0.46      0.42        65\n",
            "\n",
            "Test Set Confusion Matrix for 1960s_gas:\n",
            " [[ 3  2  7]\n",
            " [ 3  3 14]\n",
            " [ 3  6 24]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n",
            "Cross-Validation Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.98      0.93      0.96        46\n",
            "     neutral       0.95      0.85      0.90        46\n",
            "    positive       0.81      0.93      0.87        46\n",
            "\n",
            "    accuracy                           0.91       138\n",
            "   macro avg       0.91      0.91      0.91       138\n",
            "weighted avg       0.91      0.91      0.91       138\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1970s_gas:\n",
            " [[43  0  3]\n",
            " [ 0 39  7]\n",
            " [ 1  2 43]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.33      0.50         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.64      1.00      0.78         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.55      0.44      0.43        15\n",
            "weighted avg       0.59      0.67      0.57        15\n",
            "\n",
            "Test Set Confusion Matrix for 1970s_gas:\n",
            " [[1 0 2]\n",
            " [0 0 3]\n",
            " [0 0 9]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n",
            "Cross-Validation Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.53      0.53      0.53        75\n",
            "     neutral       0.92      0.80      0.86        75\n",
            "    positive       0.51      0.57      0.54        75\n",
            "\n",
            "    accuracy                           0.64       225\n",
            "   macro avg       0.65      0.64      0.64       225\n",
            "weighted avg       0.65      0.64      0.64       225\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1980s_gas:\n",
            " [[40  2 33]\n",
            " [ 7 60  8]\n",
            " [29  3 43]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.55      0.46      0.50        13\n",
            "     neutral       0.00      0.00      0.00         7\n",
            "    positive       0.38      0.62      0.47        13\n",
            "\n",
            "    accuracy                           0.42        33\n",
            "   macro avg       0.31      0.36      0.32        33\n",
            "weighted avg       0.36      0.42      0.38        33\n",
            "\n",
            "Test Set Confusion Matrix for 1980s_gas:\n",
            " [[6 0 7]\n",
            " [1 0 6]\n",
            " [4 1 8]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n",
            "Cross-Validation Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.86      0.80        21\n",
            "     neutral       0.95      0.86      0.90        21\n",
            "    positive       0.85      0.81      0.83        21\n",
            "\n",
            "    accuracy                           0.84        63\n",
            "   macro avg       0.85      0.84      0.84        63\n",
            "weighted avg       0.85      0.84      0.84        63\n",
            "\n",
            "Cross-Validation Confusion Matrix for 1990s_gas:\n",
            " [[18  0  3]\n",
            " [ 3 18  0]\n",
            " [ 3  1 17]]\n",
            "--------------------------------------------------\n",
            "Test Set Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.75      0.60         4\n",
            "     neutral       0.00      0.00      0.00         2\n",
            "    positive       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.50         8\n",
            "   macro avg       0.33      0.42      0.37         8\n",
            "weighted avg       0.38      0.50      0.42         8\n",
            "\n",
            "Test Set Confusion Matrix for 1990s_gas:\n",
            " [[3 0 1]\n",
            " [2 0 0]\n",
            " [1 0 1]]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RobBERT + Pattern.nl"
      ],
      "metadata": {
        "id": "aZiBZH6Cbh_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install transformers\n",
        "!pip install imbalanced-learn\n",
        "!pip install torch\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1yfoGZsdrkf",
        "outputId": "1464ee1c-06bc-4d31-8c5a-7b707b0e2357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.4)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=960b119e86579f1c0fc33cf125dc5bb1b98419050a5575586d37222f57faba11\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124734 sha256=22ecfdec557c15d8a57504a2cf07db6df4c932d06293de45b2619aeb2b6a9898\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=3de6b80a6a8395a3ffdddfafd44a8aaea686627ac92dc3d6b19a506c17c80170\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.1 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.5.1 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from datasets import Dataset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from pattern.nl import sentiment\n",
        "\n",
        "# Function to set all seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Setting the seed\n",
        "set_seed(42)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Check if only the labels 0, 1 and 2 are present\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return 0  # negative\n",
        "    elif label == 1:\n",
        "        return 1  # neutral\n",
        "    elif label == 2:\n",
        "        return 2  # positive\n",
        "    else:\n",
        "        return -1  # unknown\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories for the lexicon approach\n",
        "def map_labels_lexicon(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Function to tokenize the texts\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Function to perform lexicon-based sentiment analysis\n",
        "def lexicon_sentiment_analysis(texts):\n",
        "    sentiment_scores = []\n",
        "    for text in texts:\n",
        "        polarity, _ = sentiment(text)\n",
        "        sentiment_scores.append(polarity)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"pdelobelle/robbert-v2-dutch-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Define a custom model class to accept additional lexicon score input\n",
        "class RobertaWithLexicon(RobertaForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.lexicon_fc = nn.Linear(1, config.num_labels)  # Map lexicon score to the same output space\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, lexicon_score=None, labels=None):\n",
        "        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if lexicon_score is not None:\n",
        "            lexicon_score = lexicon_score.unsqueeze(1).to(logits.device)\n",
        "            lexicon_logits = self.lexicon_fc(lexicon_score)  # Map lexicon score to logits\n",
        "            logits += lexicon_logits  # Add lexicon logits to BERT logits\n",
        "\n",
        "        return (outputs.loss, logits) if labels is not None else logits\n",
        "\n",
        "# Initialize the custom model\n",
        "model = RobertaWithLexicon.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Function to safely add the lexicon scores\n",
        "def add_lexicon_score(dataset, scores):\n",
        "    if 'lexicon_score' in dataset.column_names:\n",
        "        dataset = dataset.remove_columns(['lexicon_score'])\n",
        "    scores = scores.tolist()  # Convert to list if not already\n",
        "    return dataset.add_column('lexicon_score', scores)\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Perform lexicon sentiment analysis and add the sentiment scores to the dataframe\n",
        "    lexicon_scores = lexicon_sentiment_analysis(X)\n",
        "    df = pd.DataFrame({'text': X, 'label': y, 'lexicon_score': lexicon_scores})\n",
        "\n",
        "    train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=0.1765, random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "    # Oversample the training data to handle class imbalance\n",
        "    oversampler = RandomOverSampler(random_state=42)\n",
        "    train_df_resampled, train_labels_resampled = oversampler.fit_resample(train_df[['text', 'lexicon_score']], train_df['label'])\n",
        "    train_df_resampled['label'] = train_labels_resampled\n",
        "\n",
        "    # Convert pandas DataFrames to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df_resampled)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Add lexicon scores as additional feature\n",
        "    train_dataset = add_lexicon_score(train_dataset, train_df_resampled['lexicon_score'])\n",
        "    val_dataset = add_lexicon_score(val_dataset, val_df['lexicon_score'])\n",
        "    test_dataset = add_lexicon_score(test_dataset, test_df['lexicon_score'])\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results/{dataset_name}',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f'./logs/{dataset_name}',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        learning_rate=1e-4,\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Evaluating {dataset_name}...\")\n",
        "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
        "    print(f\"Test Set Evaluation for {dataset_name}:\\n\", eval_result)\n",
        "\n",
        "    # Get predictions from the model\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    preds = predictions.predictions.argmax(-1)\n",
        "    true_labels = test_dataset['label']\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(true_labels, preds, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
        "    print(f\"Classification Report for {dataset_name}:\\n\", report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, preds)\n",
        "    print(f\"Confusion Matrix for {dataset_name}:\\n\", cm)\n",
        "\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9bea1e739f3744c79f22a316c0043043",
            "480be96bf25549e0bf24c1f2762cbaa9",
            "62421c028ebe4155a612a4c989b785ad",
            "5c9193d4d78e404bbe9a0d382fd8c264",
            "df6d36c22b83484986bad10ff72e9d16",
            "3e60d91a22904939a9cdcf8d42a5bd7e",
            "c7a58b1b17734707b882de0120f5e4d9",
            "39627d20c8ba43398939fe24fe19dc2c",
            "729ac4a6b7264f02ab4225485d8e505d",
            "1268357c709948c1ae0da8cf815e85e6",
            "965aa2ac070449d894a7854b3e6eaf65",
            "55f73317842746af875ba46301adb48c",
            "af311eb416d34cbaa14bf9ab4e74f534",
            "3b912d65adde4be582aaefbac5bd739e",
            "670ca45d1e254b9e9dff5d180f5b7d36",
            "e1491eb1a0274be9b6281e893cfc758f",
            "0d9db63d0c184712b27e8da7cf7ab015",
            "84f41f73e2c744b19fa447a86f515d9e",
            "9d9a1f3349c24674b5510228e39a9856",
            "61ee2cdec54148ab86f4662bfae5a629",
            "c94fd9763a854fc19084d94b4a90fb72",
            "05f95a61794b43479b22b35a4f7b4cb3",
            "0beb0168352545de92293f51275dba92",
            "47a5d10d7f93478e8047d3a455856e4c",
            "2cf2acf8d5354013b1b15200e38a5d6f",
            "b6dd5011df3a474494ff6b7615504c11",
            "ca6405769dfd48f9b994f8e396125c74",
            "d0b3a6c5a9204f90bd2510737cee8337",
            "ea47c72d609342508ea56b5b12574441",
            "ee3036424c2a4ce48f2a3575ae8e3b0b",
            "fdfa76317c8c4e2ca5ede996a5e9a50e",
            "34ac2e4884984385b7885bed4c24be6f",
            "99b134d6b69b4e7780c4edc7285987ca",
            "7ec05aefec3144e4a72d1f9bae127f52",
            "bacb5abde9b14951973cffae75b6d434",
            "54bc156827c0425f98e69cba20cd40a3",
            "65771a7a9639424bbe95c5f37aea17d2",
            "558b04a9063d4992a68bf8e28d31a028",
            "f32de0d256f14a319e382ef9d19035ab",
            "83122de03c7b4a3faf3b46c892ffdafd",
            "7ada476d3d824ac69060e56a0d3e267f",
            "e78bbdce16e24e05af6be4b83b070a9d",
            "97955c706f9f4c10ac70c5c88c7f9a3f",
            "cb4e9d2524d147729a9c506b28beec47",
            "fc94ca2be535437aa25197f3241dd9e8",
            "9d5c6ae7a4e74c53ac5e41071f229623",
            "97d74c98fd9e4dc5bb41d5f19e06b22a",
            "374971e041d44240a488a475611d4f13",
            "939281955b8543199da218a938297739",
            "0e265359ac8c4fb0b2b579c52deaa54b",
            "6983d6e1441a4c48bae6982b6723a143",
            "b34789ff8f6a462da3ed99bec04f6384",
            "e62b40b144cb46f49ac02e5b4967423d",
            "9b1eb3b509e24d23b8ae825fd107c781",
            "ba8d6ff7959e45829e6ac57f5e6b2c5f",
            "d3197d995b324926a8367918d9eca315",
            "a3516841db5246f3aff598798af4e813",
            "b46020142be14625b9eaa54d0f34a393",
            "fe0695aa3a3946788dba65ad8a187ed5",
            "839d0b8119d449d895cbe30eab265c2f",
            "04ca725f87ca474b9124165443353360",
            "04ea9aefbfa44a32bad54235ea606870",
            "48c5225e8986450393ce92bad226d96b",
            "55f75a1899f74f38b5adea71e0780850",
            "2319ad7358584f569918f247d62dd044",
            "bc312bf1b8fc4eee9d590d842cd30365",
            "00013aac155c4d60ae6aeef5af483d6e",
            "2690b3c4dc1e43079b3a4ba35d7c86d5",
            "cd2ccc61d04e49e88fd4b57117ca06a4",
            "d988c75ab47a4506839e39a0721e849b",
            "313b4cea90a849ec8883a484de792da9",
            "cb3e9a438d6743d980addeecdeccb4b4",
            "9f40b391e2cb46c8a3de2a8cda098883",
            "87fe0d7cd42444949b30ac59252f3cec",
            "9a35e9f60a354970aaba092588d3ad9f",
            "4c9b1c604fe64bfa8e57c3b293cdfec5",
            "311fabf858ed4f7c8eaf62937e234b6e",
            "dfe51ac0cd9d4aa591cda3c51da0e5a7",
            "d7495bd6ea39450abe7072e8c2bef7d6",
            "3f70352bb3074b7bb41691cc2e3f3c1f",
            "6ab6320f029a4871b76c4889321e55a7",
            "e176176698ec443795a5aa4b5ebfe3a7",
            "01e56dc583ab4fb0b850d783bbda7d68",
            "104ccd838abd4f45904ce6390a040936",
            "a56d0755552a4a268b72cf10f0cab66d",
            "e384e2ae1e87409cbc20f289cd83793f",
            "b187ebcfba38483a86203d67caedca93",
            "2910c390a93646979880f22a6e5255db",
            "bdcd300abd4240b3acd97f1089c74658",
            "ab07ec0ddd044770a227495c2d46a28f",
            "06eb1511a7d64d96ac91f5d661c89286",
            "15a410772092498f9bc1e3e294e81a58",
            "2292bef7875741e0a1f6def91fe02ff4",
            "8894144df51a4cd69b30cac8b1b23f0b",
            "8c7894d7e2fb45f795a8299804885288",
            "9e26dbf4f97d4368b9042e13f102c2c6",
            "6d4a580e71134d549c81085ebe96d245",
            "ddd8f903cdc94eecac2369931a45e80e",
            "e17e94b1fdb84d898119748f233c375c",
            "5de0e131b4c5413bb1e36d2e83a53db9",
            "c81e42787a7748ae883f5ac0122adfed",
            "a92cff7f55704e348b2dd593166ab181",
            "40aec6dc954a4b38a1c3a69eca27cd7e",
            "1150de9287b64cfdaca67398a0527027",
            "ab3b6892f9eb4e42be6683c27fc8bb90",
            "909439c6b5e14c54a52c7116e4a35f59",
            "31380d7472bd4b87b5e56f1a344b8408",
            "e42f744f21dd4f7db88c512886844520",
            "13f946d6aeb943d98628f3c380829fef",
            "1e771176d2fa4f7792f391a29af66f35",
            "caae3a46f723428c86d8cd3d15414493",
            "5a075ce1cb6b4f62b695113dc90ffbfe",
            "9ebf68d121ba4beba056a0dedc21ca48",
            "150c3f860d6b41bf98ba92ef892211fb",
            "53c6e9912e2b4dfaad844a43245f6222",
            "08abe1588b564a5896f6fd55c5225388",
            "5d6676eec6164675b022e99a659b6954",
            "ee56e98772d24861a5e6fb70dc36f392",
            "3f95f9acb3214a78b684414220939299",
            "07315b88ac9d4544a82c33eca758a15f",
            "f89f01b3fa4543869aba5e49a1593a51",
            "e9f3bd06944c414b91b32196a85e549b",
            "fb01ffe55692478dac16e0a8fac7f2aa",
            "ca1afaa23171450ab5ec1bc96a2c7719",
            "041de0796a7d448e911bcb275aee4407",
            "d82647e255bd4dedaea40456658c5fc2",
            "4162895b31bd4eb0b71d717c95c0d486",
            "5367a28b0df34bd7af220716f55c6a74",
            "c18900bd97864a49a250baf88a48b907",
            "97f076d1cbda4843af060990de8820b2",
            "f100b8834eb54645af84f86d5473ada6",
            "913d13785f2d4d5bb361089f83964c25"
          ]
        },
        "id": "85r4aPdqAAAt",
        "outputId": "e92bf16d-1eaa-4212-f551-855e03fc2fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaWithLexicon were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'lexicon_fc.bias', 'lexicon_fc.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bea1e739f3744c79f22a316c0043043"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55f73317842746af875ba46301adb48c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0beb0168352545de92293f51275dba92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/116 04:39, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.197000</td>\n",
              "      <td>1.155904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.682200</td>\n",
              "      <td>1.133945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.350300</td>\n",
              "      <td>1.337193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.094200</td>\n",
              "      <td>1.450952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1960s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1960s_gas:\n",
            " {'eval_loss': 1.0511798858642578, 'eval_runtime': 1.9832, 'eval_samples_per_second': 32.775, 'eval_steps_per_second': 4.538, 'epoch': 4.0}\n",
            "Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      0.75      0.56        12\n",
            "     neutral       0.00      0.00      0.00        20\n",
            "    positive       0.57      0.76      0.65        33\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.34      0.50      0.40        65\n",
            "weighted avg       0.37      0.52      0.43        65\n",
            "\n",
            "Confusion Matrix for 1960s_gas:\n",
            " [[ 9  0  3]\n",
            " [ 4  0 16]\n",
            " [ 7  1 25]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ec05aefec3144e4a72d1f9bae127f52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc94ca2be535437aa25197f3241dd9e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3197d995b324926a8367918d9eca315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 02:38, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.091561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.044900</td>\n",
              "      <td>1.457587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.374500</td>\n",
              "      <td>1.907759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.220600</td>\n",
              "      <td>1.756206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1970s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1970s_gas:\n",
            " {'eval_loss': 0.8900591135025024, 'eval_runtime': 0.5172, 'eval_samples_per_second': 29.005, 'eval_steps_per_second': 3.867, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.33      0.50         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.64      1.00      0.78         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.55      0.44      0.43        15\n",
            "weighted avg       0.59      0.67      0.57        15\n",
            "\n",
            "Confusion Matrix for 1970s_gas:\n",
            " [[1 0 2]\n",
            " [0 0 3]\n",
            " [0 0 9]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00013aac155c4d60ae6aeef5af483d6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfe51ac0cd9d4aa591cda3c51da0e5a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdcd300abd4240b3acd97f1089c74658"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 03:00, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.185300</td>\n",
              "      <td>1.078205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.075500</td>\n",
              "      <td>1.107207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.984100</td>\n",
              "      <td>1.146729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.811300</td>\n",
              "      <td>1.131910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1980s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1980s_gas:\n",
            " {'eval_loss': 1.076548457145691, 'eval_runtime': 1.0676, 'eval_samples_per_second': 30.912, 'eval_steps_per_second': 4.684, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.45      0.77      0.57        13\n",
            "     neutral       0.00      0.00      0.00         7\n",
            "    positive       0.36      0.31      0.33        13\n",
            "\n",
            "    accuracy                           0.42        33\n",
            "   macro avg       0.27      0.36      0.30        33\n",
            "weighted avg       0.32      0.42      0.36        33\n",
            "\n",
            "Confusion Matrix for 1980s_gas:\n",
            " [[10  0  3]\n",
            " [ 3  0  4]\n",
            " [ 9  0  4]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5de0e131b4c5413bb1e36d2e83a53db9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caae3a46f723428c86d8cd3d15414493"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9f3bd06944c414b91b32196a85e549b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 02:09, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.083399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.103045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.032700</td>\n",
              "      <td>1.073438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.032700</td>\n",
              "      <td>1.074149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1990s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1990s_gas:\n",
            " {'eval_loss': 1.0107314586639404, 'eval_runtime': 0.2742, 'eval_samples_per_second': 29.172, 'eval_steps_per_second': 3.646, 'epoch': 4.0}\n",
            "Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.50      0.50         4\n",
            "     neutral       0.50      0.50      0.50         2\n",
            "    positive       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.38         8\n",
            "   macro avg       0.33      0.33      0.33         8\n",
            "weighted avg       0.38      0.38      0.38         8\n",
            "\n",
            "Confusion Matrix for 1990s_gas:\n",
            " [[2 0 2]\n",
            " [1 1 0]\n",
            " [1 1 0]]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTje + Pattern.nl"
      ],
      "metadata": {
        "id": "6-9cMmX7FeEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install imbalanced-learn\n",
        "!pip install torch\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H11etQQIFjSR",
        "outputId": "e0a8670c-5347-4672-f122-c9493c0e2edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.9.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.4)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=867e59eee7534ce780be3dfa36e9b5e251969f9d0e0c507b9bc99e98692dcde4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124733 sha256=f90593655957738f7b8d4630047c6395e60431388e8f5ffbbff61345462a7159\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=81a036d705be2d266c8bf026f6d685fa0c43b4294f4f4f8b1a3036057e06f39e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.9.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.0 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.5.1 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from datasets import Dataset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from pattern.nl import sentiment\n",
        "\n",
        "# Function to set all seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Setting the seed\n",
        "set_seed(42)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Check if only the labels 0, 1 and 2 are present\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return 0  # negative\n",
        "    elif label == 1:\n",
        "        return 1  # neutral\n",
        "    elif label == 2:\n",
        "        return 2  # positive\n",
        "    else:\n",
        "        return -1  # unknown\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories for the lexicon approach\n",
        "def map_labels_lexicon(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Function to tokenize the texts\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Function to perform lexicon-based sentiment analysis\n",
        "def lexicon_sentiment_analysis(texts):\n",
        "    sentiment_scores = []\n",
        "    for text in texts:\n",
        "        polarity, _ = sentiment(text)\n",
        "        sentiment_scores.append(polarity)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"wietsedv/bert-base-dutch-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Define a custom model class to accept additional lexicon score input\n",
        "class BertWithLexicon(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.lexicon_fc = nn.Linear(1, config.num_labels)  # Map lexicon score to the same output space\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, lexicon_score=None, labels=None):\n",
        "        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if lexicon_score is not None:\n",
        "            lexicon_score = lexicon_score.unsqueeze(1).to(logits.device)\n",
        "            lexicon_logits = self.lexicon_fc(lexicon_score)  # Map lexicon score to logits\n",
        "            logits += lexicon_logits  # Add lexicon logits to BERT logits\n",
        "\n",
        "        return (outputs.loss, logits) if labels is not None else logits\n",
        "\n",
        "# Initialize the custom model\n",
        "model = BertWithLexicon.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Function to safely add the lexicon scores\n",
        "def add_lexicon_score(dataset, scores):\n",
        "    if 'lexicon_score' in dataset.column_names:\n",
        "        dataset = dataset.remove_columns(['lexicon_score'])\n",
        "    scores = scores.tolist()  # Convert to list if not already\n",
        "    return dataset.add_column('lexicon_score', scores)\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Perform lexicon sentiment analysis and add the sentiment scores to the dataframe\n",
        "    lexicon_scores = lexicon_sentiment_analysis(X)\n",
        "    df = pd.DataFrame({'text': X, 'label': y, 'lexicon_score': lexicon_scores})\n",
        "\n",
        "    train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=0.1765, random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "    # Oversample the training data to handle class imbalance\n",
        "    oversampler = RandomOverSampler(random_state=42)\n",
        "    train_df_resampled, train_labels_resampled = oversampler.fit_resample(train_df[['text', 'lexicon_score']], train_df['label'])\n",
        "    train_df_resampled['label'] = train_labels_resampled\n",
        "\n",
        "    # Convert pandas DataFrames to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df_resampled)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Add lexicon scores as additional feature\n",
        "    train_dataset = add_lexicon_score(train_dataset, train_df_resampled['lexicon_score'])\n",
        "    val_dataset = add_lexicon_score(val_dataset, val_df['lexicon_score'])\n",
        "    test_dataset = add_lexicon_score(test_dataset, test_df['lexicon_score'])\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results/{dataset_name}',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f'./logs/{dataset_name}',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        learning_rate=1e-4,\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Evaluating {dataset_name}...\")\n",
        "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
        "    print(f\"Test Set Evaluation for {dataset_name}:\\n\", eval_result)\n",
        "\n",
        "    # Get predictions from the model\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    preds = predictions.predictions.argmax(-1)\n",
        "    true_labels = test_dataset['label']\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(true_labels, preds, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
        "    print(f\"Classification Report for {dataset_name}:\\n\", report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, preds)\n",
        "    print(f\"Confusion Matrix for {dataset_name}:\\n\", cm)\n",
        "\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88863feae9784e5692dfbd9701f93383",
            "8dbfdaccd1704ba793e396e084996029",
            "58d32159e17f40aba136476317813a62",
            "c3ec9e721ce94a66aa22e514181e0dfc",
            "abec59ed9f974184941f121ac30630ed",
            "e708b2c1f77b4bd688ded69d1f803dec",
            "37ffda5cf0864c80b4deb80a2a3f796e",
            "06a04244e7cb4c41beaa4f844c8da3a3",
            "393fbe3a5a074705b3279c349be0f663",
            "ddfe77aec7e547e18dab34db781c8283",
            "4649a4a4bc894bc5938c7c2f68e5926b",
            "a712965e7e364ab2a7083707becc090d",
            "62bfba97519f417187501b3f1cedf354",
            "52f91ccfdcec4c3ab37657943bc31925",
            "83b533d0c56345c79874a9d16cd35ca4",
            "56c1225a81ec405c8df08b33cdd851d5",
            "481660e82cdb4f8a9ad7cda6d0e40af0",
            "dacb5be64f11421197b047b5ac6276e4",
            "0ca6c8da557c4e669a637eb7214a788d",
            "de673ab53e3a468ba6fa23612c5d8d9a",
            "a0ab60ae9b4c4d29893fbc43389fc4eb",
            "c78ead6eae7b497da7fdcad86243d037",
            "e6468b6c6ceb4602b191fdf2a8ff7254",
            "954b0c495a4841fd85f76570bbab6665",
            "588471b81c254701bfd31378a3722542",
            "46b318ee7adb4a538af8d8d869d5a2cb",
            "43c9a404fa194f858d8948a6aa6c8ded",
            "f7949b8cfe6a4c4eac38ca256cd6bb30",
            "d47f37b2589e495dba314e8eb39b7ee4",
            "08a6031fe92a4344aa6885e3766af706",
            "b4d2290ad7dd4bbd8b8d4db0b3fc5230",
            "90611e63c2f6497fbd2a81e3af084d0d",
            "505130a58d094242bbc71196669ab3b7",
            "0ea19a25f4c74d3c861edf00542acf9d",
            "d5e70dee5e8343389a2b99ea9ddcee71",
            "9a931db70ba8451bac594e4c16a91bc0",
            "1cea4e0ec1604324934e83a748b14b20",
            "dda2f73ce9ec48be916559474e4b151b",
            "31a37c26e05d4602a5938aeb71b5d38a",
            "7a3e48417fc34d1cbaf8850d958f10ed",
            "c1f1c62600ad4d5392e15059ccca6801",
            "0c7582bc501d447284fddd028f7fd1ee",
            "85d42b446a5d4224a44e2d25db289dfb",
            "c5bc635167ca43f982433ec19e9b16e6",
            "a418c8ed5ad549c5b7423019bfbd1f9f",
            "932aed9139654a668d4ede7855f2fc79",
            "c411d48cb3184f7b81da6123e43ff644",
            "612231921f3c4f8aa52d33bb8d05deb2",
            "17e88d9941ab4f438a15841ed832660a",
            "db23dee863f94a5e83b4519b9fc23f3c",
            "73828163681c4c409e5bb3bc1bae9f7c",
            "6e8d4be1ae244e559eefee874efee39a",
            "b5265bfc9d264d4991962a52c0fd5ae0",
            "8e4b8b97a65a40269054f03fc4d033a4",
            "124ab7d589384142aad6c26121f08bca",
            "c860959c5b5b4268a97966c88e1c1d0c",
            "b133d769e986415a9232959ee9c3bb1e",
            "36fdf2f27a044dcc97552037d94730bd",
            "9451eb102a9e4f209d6a84d7ec8fe524",
            "9df5f2fccad04b98b78c9d42c574b8b4",
            "850f59a51f474154a2bd2178924ea697",
            "66dbf5ae515c41efaa16389a84155f56",
            "3c39ea6088844147b50ff6bd6fc0aa6b",
            "0a679a56b5ed473b8d986b7a52e97cfd",
            "a4e4724366e442c18695fdd601fac080",
            "5e4211e2596644769b3bedadf6f17167",
            "de2a3b79d75143ee8c727522d5a52f1b",
            "018b2d20392144f781f4c5ac39f12fc9",
            "b8d1d695b6c942849d4c5933422229f7",
            "5a80cc3a690f48c3a0f8df79b8c6bedd",
            "2b9da8d8288a427ea997daa9c2c69641",
            "bda1a6f1a1604bb8ad6f5f769b0c7e31",
            "cfca505609e04f4ebdae4be4acabfa28",
            "68ee102599ae4fbcb94ff22a0351b817",
            "cfb6b6286b4d4f5997d43041e4f63e95",
            "2109f51621474a1eba45dfdedcb14c46",
            "1fcde90e44f44027a45898160ed722c3",
            "1b5e90cb02474c869186c18ee1bf7583",
            "7978761b7311468d944591d74bb041e6",
            "a2df8db6a40047d49a2ec9d42e90b7a6",
            "e0f54dd2374845e18fa526fd6ed107a6",
            "84135d1853504b7fa95f452f7df4e1fa",
            "8fd6d270ea1b4d1bbdc6341abdc98e03",
            "75e23cbba3d740f9a2f709d3d8bcd0e1",
            "56e52561112b498b98db8100ef5182e4",
            "96f5ae6294c44ad7a743778b14e7fdf8",
            "2f3f4c32508847c89006c4411eb19eab",
            "70a82b6809c740aa9e9776b2368e0471",
            "f464e11f295448718c9a22806ba40e9c",
            "44aae311218e421296b1af7c17796b1f",
            "b592e8c732df4bb68be46c4e626e0bdf",
            "ad51c237d5664d599e8eccc76dd54c5b",
            "974e5c199c064288afd99c2ffd0ca748",
            "5933f5d067a74e2492e279cb7f2b33a2",
            "7fb0d4ab339d495182bb67619620ce8b",
            "105e94b3e6e342c383cff748421cb60f",
            "db049f0d6cd047679c7506f33d55ef8a",
            "36ffbc707d5e472198a7a561f3355f70",
            "0899b80d8414411d8a896c07e847d08e",
            "9b568501583d4a529876142a37b6384c",
            "8e7427de40a0401195567fe1eac42cee",
            "43aa4b7f4d134e7983814f4b8fc1e0ec",
            "7d6f0f6c120544adaaec2ae8203bd255",
            "55806c09fd814333876d06ff51392fc7",
            "d6747883ee0f4841bc21d1fe2f7a8b5a",
            "e4e183191c4c462799add605dfe2d559",
            "10aae2296bfd4599818ef97d40bd63c9",
            "fcfb3fdefd644c898c9fa7f79b087792",
            "802907337bab4c51b15e0b3d30e88152",
            "a9dd7935883b4ddfa769c7bd49b9535a",
            "24939b77a66b4c94bb16b63eb182f2f4",
            "784760882a064c8fb56b54b6fd767b9b",
            "a304b7e4ce484b27a2797c948fcc50f6",
            "12a382e2429e4d90ab7a560dc6d65cbe",
            "10459272322d4a76a042a78c8a8ed0d1",
            "0270fcdfb23344409301f9ff90f3b714",
            "fd6cc9ba3c724cd588b65c9b9f436946",
            "2de15581e1024ce9b9a90b9a8f5cb67f",
            "08fad744c3454147b2a4c69c0fd65088",
            "c3c543de6e944c478b8df46b9a7f9cff",
            "264f7d4e723c4b0b882dd45d6ff031d0",
            "4537d6742d774641ac287fcf3a0db4be",
            "bb6b7e7786d449949f7c8ad249310d45",
            "1542bd19b6dc476e9cdce3b7bf67b87f",
            "6b6f9903ec1642d6b7a02183cdbdf872",
            "a6b646e5195a48b182e584bc241bb04f",
            "8b766a7ab5dd452497aed02e9bd40c9f",
            "4e8f3515d53c4135a1c162f074b869d3",
            "d282747209ae4727beea506918d0539d",
            "a964dee615fe4ab8a9c5258c7dad3e4a",
            "2a40413f4dab4116a42d54fe916c09d5",
            "d9053badd7f24f2ea9c0e8addebaa8e6"
          ]
        },
        "id": "Q_C9_1Xq_IBO",
        "outputId": "48d482fd-23bd-423c-db90-5f2fa1324e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertWithLexicon were not initialized from the model checkpoint at wietsedv/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'lexicon_fc.bias', 'lexicon_fc.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88863feae9784e5692dfbd9701f93383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a712965e7e364ab2a7083707becc090d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6468b6c6ceb4602b191fdf2a8ff7254"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/116 04:27, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.124200</td>\n",
              "      <td>1.128516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.452400</td>\n",
              "      <td>1.013469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.141300</td>\n",
              "      <td>1.315779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>1.564219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1960s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1960s_gas:\n",
            " {'eval_loss': 1.3837627172470093, 'eval_runtime': 1.8229, 'eval_samples_per_second': 35.657, 'eval_steps_per_second': 4.937, 'epoch': 4.0}\n",
            "Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.17      0.22        12\n",
            "     neutral       0.00      0.00      0.00        20\n",
            "    positive       0.51      0.85      0.64        33\n",
            "\n",
            "    accuracy                           0.46        65\n",
            "   macro avg       0.28      0.34      0.29        65\n",
            "weighted avg       0.32      0.46      0.36        65\n",
            "\n",
            "Confusion Matrix for 1960s_gas:\n",
            " [[ 2  1  9]\n",
            " [ 2  0 18]\n",
            " [ 2  3 28]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ea19a25f4c74d3c861edf00542acf9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a418c8ed5ad549c5b7423019bfbd1f9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c860959c5b5b4268a97966c88e1c1d0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 03:09, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.172462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.008000</td>\n",
              "      <td>1.619502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.210600</td>\n",
              "      <td>1.842738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>2.056618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1970s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1970s_gas:\n",
            " {'eval_loss': 0.9468898177146912, 'eval_runtime': 0.4967, 'eval_samples_per_second': 30.199, 'eval_steps_per_second': 4.026, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.33      0.50         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.64      1.00      0.78         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.55      0.44      0.43        15\n",
            "weighted avg       0.59      0.67      0.57        15\n",
            "\n",
            "Confusion Matrix for 1970s_gas:\n",
            " [[1 0 2]\n",
            " [0 0 3]\n",
            " [0 0 9]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de2a3b79d75143ee8c727522d5a52f1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b5e90cb02474c869186c18ee1bf7583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f464e11f295448718c9a22806ba40e9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 02:38, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.105900</td>\n",
              "      <td>1.202203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.681200</td>\n",
              "      <td>1.397081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.282000</td>\n",
              "      <td>1.657870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.097300</td>\n",
              "      <td>1.862386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1980s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1980s_gas:\n",
            " {'eval_loss': 1.2074679136276245, 'eval_runtime': 0.9958, 'eval_samples_per_second': 33.139, 'eval_steps_per_second': 5.021, 'epoch': 4.0}\n",
            "Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.23      0.32        13\n",
            "     neutral       0.29      0.71      0.42         7\n",
            "    positive       0.40      0.31      0.35        13\n",
            "\n",
            "    accuracy                           0.36        33\n",
            "   macro avg       0.40      0.42      0.36        33\n",
            "weighted avg       0.42      0.36      0.35        33\n",
            "\n",
            "Confusion Matrix for 1980s_gas:\n",
            " [[3 5 5]\n",
            " [1 5 1]\n",
            " [2 7 4]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b568501583d4a529876142a37b6384c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24939b77a66b4c94bb16b63eb182f2f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4537d6742d774641ac287fcf3a0db4be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 01:51, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.259775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.440391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.697200</td>\n",
              "      <td>1.602657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.697200</td>\n",
              "      <td>1.728466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 1990s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Evaluation for 1990s_gas:\n",
            " {'eval_loss': 1.1769353151321411, 'eval_runtime': 0.2833, 'eval_samples_per_second': 28.243, 'eval_steps_per_second': 3.53, 'epoch': 4.0}\n",
            "Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.50      0.50         4\n",
            "     neutral       0.00      0.00      0.00         2\n",
            "    positive       0.25      0.50      0.33         2\n",
            "\n",
            "    accuracy                           0.38         8\n",
            "   macro avg       0.25      0.33      0.28         8\n",
            "weighted avg       0.31      0.38      0.33         8\n",
            "\n",
            "Confusion Matrix for 1990s_gas:\n",
            " [[2 0 2]\n",
            " [1 0 1]\n",
            " [1 0 1]]\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RobBERT + Pattern.nl + SVM"
      ],
      "metadata": {
        "id": "OXr6qLmkNqHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install imbalanced-learn\n",
        "!pip install torch\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install pattern"
      ],
      "metadata": {
        "id": "5l5U7ZjJN3sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3dc5277-01b1-4bec-95d9-772180be8dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.6.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.4)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=c5233c05adcdde4abe491ce55bb809129eb6b1a3d09f75cbf674cdd7126dbb8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124737 sha256=3669d61cfcfda0609c55147247a1c719a2022fdb72f1b0ee3ecdbcfccf19095d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=3381151de3f5984bbca7e61c3ab0ddd208cefa326b5ba39719ff7f9e6f3f2df6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.0 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.5.1 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from datasets import Dataset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from pattern.nl import sentiment\n",
        "\n",
        "# Function to set all seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Setting the seed\n",
        "set_seed(42)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Check if only the labels 0, 1 and 2 are present\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return 0  # negative\n",
        "    elif label == 1:\n",
        "        return 1  # neutral\n",
        "    elif label == 2:\n",
        "        return 2  # positive\n",
        "    else:\n",
        "        return -1  # unknown\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories for the lexicon approach\n",
        "def map_labels_lexicon(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Function to tokenize the texts\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Function to perform lexicon-based sentiment analysis\n",
        "def lexicon_sentiment_analysis(texts):\n",
        "    sentiment_scores = []\n",
        "    for text in texts:\n",
        "        polarity, _ = sentiment(text)\n",
        "        sentiment_scores.append(polarity)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"pdelobelle/robbert-v2-dutch-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Define a custom model class to accept additional lexicon score input\n",
        "class RobertaWithLexicon(RobertaForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.lexicon_fc = nn.Linear(1, config.num_labels)  # Map lexicon score to the same output space\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, lexicon_score=None, labels=None):\n",
        "        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if lexicon_score is not None:\n",
        "            lexicon_score = lexicon_score.unsqueeze(1).to(logits.device)\n",
        "            lexicon_logits = self.lexicon_fc(lexicon_score)  # Map lexicon score to logits\n",
        "            logits += lexicon_logits  # Add lexicon logits to RobBERT logits\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = outputs.loss\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# Initialize the custom model\n",
        "model = RobertaWithLexicon.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Function to safely add the lexicon scores\n",
        "def add_lexicon_score(dataset, scores):\n",
        "    if 'lexicon_score' in dataset.column_names:\n",
        "        dataset = dataset.remove_columns(['lexicon_score'])\n",
        "    scores = scores.tolist()  # Convert to list if not already\n",
        "    return dataset.add_column('lexicon_score', scores)\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Perform lexicon sentiment analysis and add the sentiment scores to the dataframe\n",
        "    lexicon_scores = lexicon_sentiment_analysis(X)\n",
        "    df = pd.DataFrame({'text': X, 'label': y, 'lexicon_score': lexicon_scores})\n",
        "\n",
        "    train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=0.1765, random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "    # Oversample the training data to handle class imbalance\n",
        "    oversampler = RandomOverSampler(random_state=42)\n",
        "    train_df_resampled, train_labels_resampled = oversampler.fit_resample(train_df[['text', 'lexicon_score']], train_df['label'])\n",
        "    train_df_resampled['label'] = train_labels_resampled\n",
        "\n",
        "    # Convert pandas DataFrames to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df_resampled)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Add lexicon scores as additional feature\n",
        "    train_dataset = add_lexicon_score(train_dataset, train_df_resampled['lexicon_score'])\n",
        "    val_dataset = add_lexicon_score(val_dataset, val_df['lexicon_score'])\n",
        "    test_dataset = add_lexicon_score(test_dataset, test_df['lexicon_score'])\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results/{dataset_name}',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f'./logs/{dataset_name}',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        learning_rate=1e-4,\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Get predictions and probabilities from the model for the training set\n",
        "    train_predictions = trainer.predict(train_dataset)\n",
        "    train_logits = train_predictions.predictions\n",
        "    train_probs = torch.softmax(torch.tensor(train_logits), dim=-1).numpy()\n",
        "\n",
        "    # Get predictions and probabilities from the model for the test set\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    test_logits = test_predictions.predictions\n",
        "    test_probs = torch.softmax(torch.tensor(test_logits), dim=-1).numpy()\n",
        "\n",
        "    # Prepare features for SVM\n",
        "    train_features = pd.DataFrame(train_probs, columns=[\"prob_neg\", \"prob_neu\", \"prob_pos\"])\n",
        "    train_features[\"lexicon_score\"] = train_df_resampled[\"lexicon_score\"].values\n",
        "    train_labels = train_df_resampled[\"label\"].values\n",
        "\n",
        "    test_features = pd.DataFrame(test_probs, columns=[\"prob_neg\", \"prob_neu\", \"prob_pos\"])\n",
        "    test_features[\"lexicon_score\"] = test_df[\"lexicon_score\"].values\n",
        "    test_labels = test_df[\"label\"].values\n",
        "\n",
        "    # Train SVM\n",
        "    svm = SVC(kernel='linear', probability=True, random_state=42)\n",
        "    svm.fit(train_features, train_labels)\n",
        "\n",
        "    # Evaluate SVM\n",
        "    svm_predictions = svm.predict(test_features)\n",
        "    report = classification_report(test_labels, svm_predictions, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
        "    print(f\"Classification Report for {dataset_name}:\\n\", report)\n",
        "\n",
        "    cm = confusion_matrix(test_labels, svm_predictions)\n",
        "    print(f\"Confusion Matrix for {dataset_name}:\\n\", cm)\n",
        "\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e549a2df39324b5f8c6168730b7e5f20",
            "4e87061357bf4df08ba1a7d71945bc79",
            "27e94ecb45d744edaf1ff99026535d42",
            "23d51fe9fd9745e8aeb9702c10caf6e1",
            "1563d8032f364cdf816d314036c515bc",
            "41332690d5c64619a36ee320c0e2fee5",
            "d07605488f8442f491fb479aae7f4615",
            "e7cbdb1735844c4c8dfdbc1121d1f7dd",
            "6467708a9b044e6ab01419daf99fa1d4",
            "8ef8c123d06140fc8b59724da4c133b7",
            "e21e95a8384c4b7ca49c96528a08b98a",
            "4be1a2a6ce844b209139769a551466a3",
            "841df14b77694f639a0562070fca3fb3",
            "3d358ac73bde4c158d62e05b847a2143",
            "b8344109e9f84bf1b16cb48b0d33481d",
            "1fef4e81e42b4dd78eaab00dfc9eb1ae",
            "7d41951cbe164519a40866666cc3f13c",
            "956685e6ff9d4fad8b4aba22b1f3ed77",
            "75d28a3b4d17490081992d79c1c347b5",
            "94e7d34482a14f409e0a7aeaef92399c",
            "c70ef284fac846108d3046ce7095483e",
            "d1710635af584575b798b359c5719b22",
            "d4647538f81b4cc0b2ee1394b2c5b557",
            "18df98b7fdbe413da528897d892da068",
            "2c1492f6a4424b14bd818766f7a71319",
            "f78b69e011b44aa2b95d9f3646d3377d",
            "2c0fd20302e44085ac9e3a5def09b0a4",
            "3630ac200c7f4701996a3f72fb3aa62f",
            "34686a094fff4320ae216ef8a6555023",
            "a9d1842fb2c34d3f9e7e65c63ba10a19",
            "79757ac96b5b45a5b7752a2056ddb469",
            "0c2a9aa9ace54d23916044803233a5cb",
            "cc62a02a8e7545978b3fd73c54ace027",
            "ce7bdd7ac797427086d423867de84847",
            "a4b6b0b6e97648f88c7af45a0488c8b2",
            "8824124fcfa048ec8452fcf2a037c214",
            "da18cd98164d4fe4ba03b03d9156162a",
            "c6d22ea40bf04a5581f0f26234e1eee0",
            "84389efd9efc4c7fa02f121418fb96f8",
            "ff035a9763ce4469b333bf418c7c63ed",
            "d54c89bb253945dda5e1d1fe8bb19f46",
            "50bbc28c73444ed5a71e0421b02b553a",
            "c9da8d5289324bfcbc9066984b88b53e",
            "ced78594fa3b44329c43958f7a339682",
            "947d7370aad4469996d9503c966dd8ed",
            "45d0bbf11044430dbd78b7b2a8bbf482",
            "8477caecde364720a6b03c29ea03332d",
            "0e68c50f99734e6498fd579ec52031f9",
            "997c91176ffb44acaaa5a1c7f22e3987",
            "8a618b2d6d4242668efd31145a64c409",
            "5b9e3daf12dd4fcaaedcf6eb4c803f14",
            "ac3593d605dd4919b57e14eaaa414e1e",
            "9d25e1833be544cf97ad0611017a1aef",
            "0c2ac649a61a4008a047a4c7c8de20d6",
            "43ea4dd8afd54a5bb4fe5509054ace1b",
            "2bed906d9003444a88ca88dfc21923aa",
            "903865af437c4ff3ae02d88e4f1ff17d",
            "2e493928000c449e8f47669a1a58be22",
            "20224e62b1ef457799e88ef0511e5b52",
            "6eda118f71744b51baa78ed672b9a1d0",
            "a1b28670c0254f61bd0336dcf49b1858",
            "9b69cc516f334bc9ab4d02ba04c82455",
            "7fa052f894bd475398c58c15c5847e73",
            "5de3bb7ad127409ca46f5f414b50c95b",
            "4d9190c8e88a4bf99fc9c529998625d9",
            "765fdfbf5968481c82989bd473f4b39b",
            "50f2ed14c98240cb8169325ef5ee4e84",
            "f47815f8773148028f4d54431b7aea86",
            "8cf378f09710476199d80e914096f9f5",
            "845010c595f640a88e23a3b1bc036f74",
            "09c13a5706274a46ba76fc0eae552f4a",
            "cac2999dd8114a7085326776ee5346fd",
            "24d23ba663bb4b86b3ec10aad02cbc25",
            "9dbfc403b9b74b078bbabe53b19e729c",
            "b3a5d1eda00c471f9de6fd2e1d7be949",
            "4bf8c26df62b4ad48b301e116b394775",
            "310e032276c747d088b513f70b08addc",
            "65ffb52f1c284dbb92cdb039702e9f0a",
            "0131846172734612bb8ea9f1615344c2",
            "c63659d10d064fa7bd570c8ad02ba8b8",
            "bcdc54c85cd348c3a5e29239a12271e2",
            "c6580015b6064d0792543804c0de33ff",
            "0b0ee994c85e432e8a6574e9184b7c2e",
            "9344d766916146cab911aaa84db5351d",
            "ef211b635d3340a0bc074cfc45db317f",
            "81ffef88b6674e86ac215b688a8c6109",
            "cb5df8df76a04a4b80bbbd855c2cfaaf",
            "d592e5c7d0ae4e94990dff8e3e0df11c",
            "d5b02df391f24f9eb1b3f187e1a45ad7",
            "885266709e404a3cb6862ed6795c14ba",
            "bc137da659024185a1f6c32c1ebbe01f",
            "63b3362098e947c19bc35ad8d4716865",
            "46056c43148142e199c0313042d5a9f4",
            "7037124e7e98485faefe1232f02f2b7f",
            "c71a4dfb716b4cc685fbd5d71419dd44",
            "d08b2ec582784cb1baf8ae4357546c86",
            "dcdefbe1e56b48918740955caac79508",
            "0d5e641159a44449867d1ddc5e792635",
            "8641ec635c6d4fb39afec253feb5e73d",
            "60240ab059b44848a7c8b62a37c6c8fe",
            "4747f548f21a441389cbdfd3d8bdcb85",
            "217f9fa2ffc740e0a18b6e57e918cc02",
            "d6454edadf204169ba45aa0991327bfb",
            "aa1742ea1ac8455392d5687ca7ba76f3",
            "275ee2853038475fb61379d343fa1e89",
            "1b5483c46fb444e3a545f3d68df8647b",
            "4827e86e557f4d388c56fbd9298cfc83",
            "5fbf5b5a97ae429a885c4fa2a5f387c1",
            "a316757bb8b846d39f2542568c9f8846",
            "b5e83f96f4e7432199f30dce43f0a8c0",
            "d04a4967279d4042911bc75470ffc644",
            "5ae571f9323f45d88d3233db201ec620",
            "86591b72b6f44ff8bc6f031e75ad03ad",
            "010688096f064ce68ac7ddbf7306c89f",
            "f1a2b830e22d4012b3d5657e876f97a5",
            "dbde2ab24db74247851b15ad1b8e0895",
            "d18df0aafb4f4c9bbc619c088f44c6b0",
            "a5f4452685144a56aa2979876173c129",
            "f26d0812318540aa87342dd4b53b9f4e",
            "68d9e535dd7142cca43c9600db52f036",
            "bbbeca1a929d4ed995ef5552537fdd58",
            "d01be99a0e4a4ab28118c1c5d86213f7",
            "f25b5f3271234474976b28c6c1d49562",
            "d13bf1cb20d9416fb3e00b9150f985bc",
            "597c0b7835aa4edfbb4f09f9632d9da8",
            "d85825c546e54436ac8849ebcd2eb79a",
            "19fad359a92240d2aaeb8cf4a6b9c3d7",
            "c3b822bafb9b45288033b298a2200de9",
            "61eb0fc8e2a7432a80807ede9bdcc867",
            "42f28e55afd64ae4accf69fae524e32f",
            "8dde915607d14a839a03d27f20430bea",
            "a884eb7f7c8349549f93ff34e05a82a3"
          ]
        },
        "id": "Fd5G1LbSEWBo",
        "outputId": "9b21783c-cc1f-4d89-a29e-c4d6b8f072ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaWithLexicon were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'lexicon_fc.bias', 'lexicon_fc.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e549a2df39324b5f8c6168730b7e5f20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4be1a2a6ce844b209139769a551466a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4647538f81b4cc0b2ee1394b2c5b557"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/116 04:52, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.197000</td>\n",
              "      <td>1.155904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.682200</td>\n",
              "      <td>1.133945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.350300</td>\n",
              "      <td>1.337193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.094200</td>\n",
              "      <td>1.450952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.44      0.58      0.50        12\n",
            "     neutral       0.33      0.25      0.29        20\n",
            "    positive       0.59      0.61      0.60        33\n",
            "\n",
            "    accuracy                           0.49        65\n",
            "   macro avg       0.45      0.48      0.46        65\n",
            "weighted avg       0.48      0.49      0.48        65\n",
            "\n",
            "Confusion Matrix for 1960s_gas:\n",
            " [[ 7  3  2]\n",
            " [ 3  5 12]\n",
            " [ 6  7 20]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce7bdd7ac797427086d423867de84847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "947d7370aad4469996d9503c966dd8ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bed906d9003444a88ca88dfc21923aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 02:42, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.091561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.044900</td>\n",
              "      <td>1.457587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.374500</td>\n",
              "      <td>1.907759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.220600</td>\n",
              "      <td>1.756206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.33      0.50         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.64      1.00      0.78         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.55      0.44      0.43        15\n",
            "weighted avg       0.59      0.67      0.57        15\n",
            "\n",
            "Confusion Matrix for 1970s_gas:\n",
            " [[1 0 2]\n",
            " [0 0 3]\n",
            " [0 0 9]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50f2ed14c98240cb8169325ef5ee4e84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65ffb52f1c284dbb92cdb039702e9f0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5b02df391f24f9eb1b3f187e1a45ad7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 02:56, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.185300</td>\n",
              "      <td>1.078205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.075500</td>\n",
              "      <td>1.107207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.984100</td>\n",
              "      <td>1.146729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.811300</td>\n",
              "      <td>1.131910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.37      0.54      0.44        13\n",
            "     neutral       0.20      0.29      0.24         7\n",
            "    positive       0.75      0.23      0.35        13\n",
            "\n",
            "    accuracy                           0.36        33\n",
            "   macro avg       0.44      0.35      0.34        33\n",
            "weighted avg       0.48      0.36      0.36        33\n",
            "\n",
            "Confusion Matrix for 1980s_gas:\n",
            " [[7 6 0]\n",
            " [4 2 1]\n",
            " [8 2 3]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60240ab059b44848a7c8b62a37c6c8fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d04a4967279d4042911bc75470ffc644"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d01be99a0e4a4ab28118c1c5d86213f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 02:29, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.083399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.103045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.032700</td>\n",
              "      <td>1.073438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.032700</td>\n",
              "      <td>1.074149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.50      0.67         4\n",
            "     neutral       0.50      0.50      0.50         2\n",
            "    positive       0.25      0.50      0.33         2\n",
            "\n",
            "    accuracy                           0.50         8\n",
            "   macro avg       0.58      0.50      0.50         8\n",
            "weighted avg       0.69      0.50      0.54         8\n",
            "\n",
            "Confusion Matrix for 1990s_gas:\n",
            " [[2 0 2]\n",
            " [0 1 1]\n",
            " [0 1 1]]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTje + Pattern.nl + SVM"
      ],
      "metadata": {
        "id": "y4TSXvFSLhgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install imbalanced-learn\n",
        "!pip install torch\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PtqrTO4LqmT",
        "outputId": "bdfdcd27-8e65-4cb8-bde2-a53e576d1d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.30.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.9.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.2.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.7.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.18.3)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=a98a5ce3a762bf9f665bb9f2789b1bf8608c775b90257ea4a087993d409433f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124731 sha256=d91cfe3937477d03c6c2b1c3683e1bd3152db36ecc271a417106337ea4a2c8a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=88edebf9f3850c3ee098a8d7bc887dd52c2bdc25d8c8ba85dbd253c7d62df246\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.9.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.0 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.5.1 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from datasets import Dataset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from pattern.nl import sentiment\n",
        "\n",
        "# Function to set all seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Setting the seed\n",
        "set_seed(42)\n",
        "\n",
        "# Function to load a single dataset\n",
        "def load_dataset(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df['text'], df['labels']\n",
        "\n",
        "# Check if only the labels 0, 1 and 2 are present\n",
        "def map_labels(label):\n",
        "    if label == 0:\n",
        "        return 0  # negative\n",
        "    elif label == 1:\n",
        "        return 1  # neutral\n",
        "    elif label == 2:\n",
        "        return 2  # positive\n",
        "    else:\n",
        "        return -1  # unknown\n",
        "\n",
        "# Replace the numerical labels with the sentiment categories for the lexicon approach\n",
        "def map_labels_lexicon(label):\n",
        "    if label == 0:\n",
        "        return \"negative\"\n",
        "    elif label == 1:\n",
        "        return \"neutral\"\n",
        "    elif label == 2:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Function to tokenize the texts\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Function to perform lexicon-based sentiment analysis\n",
        "def lexicon_sentiment_analysis(texts):\n",
        "    sentiment_scores = []\n",
        "    for text in texts:\n",
        "        polarity, _ = sentiment(text)\n",
        "        sentiment_scores.append(polarity)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"wietsedv/bert-base-dutch-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# List of datasets\n",
        "dataset_paths = [\"1960s_gas.csv\", \"1970s_gas.csv\", \"1980s_gas.csv\", \"1990s_gas.csv\"]\n",
        "\n",
        "# Define a custom model class to accept additional lexicon score input\n",
        "class BertWithLexicon(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.lexicon_fc = nn.Linear(1, config.num_labels)  # Map lexicon score to the same output space\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, lexicon_score=None, labels=None):\n",
        "        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if lexicon_score is not None:\n",
        "            lexicon_score = lexicon_score.unsqueeze(1).to(logits.device)\n",
        "            lexicon_logits = self.lexicon_fc(lexicon_score)  # Map lexicon score to logits\n",
        "            logits += lexicon_logits  # Add lexicon logits to BERT logits\n",
        "\n",
        "        return (outputs.loss, logits) if labels is not None else logits\n",
        "\n",
        "# Initialize the custom model\n",
        "model = BertWithLexicon.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Function to safely add the lexicon scores\n",
        "def add_lexicon_score(dataset, scores):\n",
        "    if 'lexicon_score' in dataset.column_names:\n",
        "        dataset = dataset.remove_columns(['lexicon_score'])\n",
        "    scores = scores.tolist()  # Convert to list if not already\n",
        "    return dataset.add_column('lexicon_score', scores)\n",
        "\n",
        "# Iterate over each dataset path in the list\n",
        "for dataset_path in dataset_paths:\n",
        "    dataset_name = dataset_path.split(\".\")[0]\n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "\n",
        "    # Load dataset\n",
        "    X, y = load_dataset(dataset_path)\n",
        "\n",
        "    # Map numerical labels to sentiment categories for ground truth\n",
        "    y = y.apply(map_labels)\n",
        "\n",
        "    # Perform lexicon sentiment analysis and add the sentiment scores to the dataframe\n",
        "    lexicon_scores = lexicon_sentiment_analysis(X)\n",
        "    df = pd.DataFrame({'text': X, 'label': y, 'lexicon_score': lexicon_scores})\n",
        "\n",
        "    train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=0.1765, random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "    # Oversample the training data to handle class imbalance\n",
        "    oversampler = RandomOverSampler(random_state=42)\n",
        "    train_df_resampled, train_labels_resampled = oversampler.fit_resample(train_df[['text', 'lexicon_score']], train_df['label'])\n",
        "    train_df_resampled['label'] = train_labels_resampled\n",
        "\n",
        "    # Convert pandas DataFrames to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df_resampled)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Add lexicon scores as additional feature\n",
        "    train_dataset = add_lexicon_score(train_dataset, train_df_resampled['lexicon_score'])\n",
        "    val_dataset = add_lexicon_score(val_dataset, val_df['lexicon_score'])\n",
        "    test_dataset = add_lexicon_score(test_dataset, test_df['lexicon_score'])\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'lexicon_score'])\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results/{dataset_name}',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f'./logs/{dataset_name}',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        learning_rate=1e-4,\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Get predictions and probabilities from the model for the training set\n",
        "    train_predictions = trainer.predict(train_dataset)\n",
        "    train_logits = train_predictions.predictions\n",
        "    train_probs = torch.softmax(torch.tensor(train_logits), dim=-1).numpy()\n",
        "\n",
        "    # Get predictions and probabilities from the model for the test set\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    test_logits = test_predictions.predictions\n",
        "    test_probs = torch.softmax(torch.tensor(test_logits), dim=-1).numpy()\n",
        "\n",
        "    # Prepare features for SVM\n",
        "    train_features = pd.DataFrame(train_probs, columns=[\"prob_neg\", \"prob_neu\", \"prob_pos\"])\n",
        "    train_features[\"lexicon_score\"] = train_df_resampled[\"lexicon_score\"].values\n",
        "    train_labels = train_df_resampled[\"label\"].values\n",
        "\n",
        "    test_features = pd.DataFrame(test_probs, columns=[\"prob_neg\", \"prob_neu\", \"prob_pos\"])\n",
        "    test_features[\"lexicon_score\"] = test_df[\"lexicon_score\"].values\n",
        "    test_labels = test_df[\"label\"].values\n",
        "\n",
        "    # Train SVM\n",
        "    svm = SVC(kernel='linear', probability=True, random_state=42)\n",
        "    svm.fit(train_features, train_labels)\n",
        "\n",
        "    # Evaluate SVM\n",
        "    svm_predictions = svm.predict(test_features)\n",
        "    report = classification_report(test_labels, svm_predictions, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
        "    print(f\"Classification Report for {dataset_name}:\\n\", report)\n",
        "\n",
        "    cm = confusion_matrix(test_labels, svm_predictions)\n",
        "    print(f\"Confusion Matrix for {dataset_name}:\\n\", cm)\n",
        "\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c62d27be21a94254b94d86a5e28a191b",
            "4161d942d4774b8db820fdbacb169b92",
            "97fd111e0b46491c867a2dcc7d4afb9c",
            "fa2c5790c0ac4fb29a8c2b525b1ee948",
            "f63907a2477f482ea1950ffe6f85f4ca",
            "d3658e8a95ca45cdba18d3b8cea2fb30",
            "9748ef4d49234cc8acba8f7823ff7913",
            "6d3483f621034184a0f72bd3339ae8f2",
            "b59f2a1b206747b29a9ebd3db1038ea7",
            "0d45907ee6344d2482a6149c67ec80e7",
            "d5250c8fe4c24b2d9ae2698c064be13d",
            "794f58e3ed614e52b6be10682056f4ab",
            "a34b8c2f17ac44728bbbd8395e990f6e",
            "8424f8056ded421998f2b22bcae900f7",
            "7ed01ba0185b43029681dc430d75b97f",
            "fcc5394486d04ef78c1f3a78b91ed335",
            "773a2759339b4fe4800d708a00442261",
            "8c2a715b1f874b60907812a941bfd58b",
            "bace75baf56a4db591b43500156f51a2",
            "4ef503b1b2574e1db647aa7c1517f561",
            "68b7d5d7d7064e1db3a124cf8c8d7b50",
            "55f3e3563375444fb679412a3dd36199",
            "02d728ff8c2d47818dc01b32bb90bc9a",
            "b6b895b4a0b443248aac26f29f0f6f43",
            "35767fea037942938ecc5195b4fbf3d5",
            "a08d6490dc34481a81e53b2a42251360",
            "591db4d9951c4944823eef5bbb17a8a7",
            "585bfa7897654e3b84c41f3714060137",
            "8f504ca711a04aaa93dadbcdef6a0e47",
            "55c21dffd8234f32a33ea62d5d00605d",
            "b58af4e388e94fc99cb9636e01abdd05",
            "35794b56a92c45a0a66851dede2aafe0",
            "261bf314e21d434c887240d112c77ad5",
            "96d04647ae0444c4b198c9e2b009d416",
            "e0b3c3c3f2684b8fa3473f52cea9ed62",
            "461aa76051bb4aa1bca26e50bda43b2a",
            "8ba012d2d72848c4998d3c3c7a25eeaa",
            "678cb6a7a25a48eca5ec3c2781053618",
            "58e0d89484e446d4b16e686707196d95",
            "e7b37b41d5a642bdb512154b03d2864e",
            "054e3d384e0a45fc9949194bd51b4d9f",
            "68a750fa2f884fe698c03761e37694ef",
            "c2de4832c0ae4afa9464e898ae89f4d9",
            "d9afc6b0adbf47d38f2dd30fc5135968",
            "a752a4f3b61547f9a9a2319829309857",
            "36a084e2dfc344ff8ba5836112484d0f",
            "32178a5c158b4bbdbe6ec8fdbc789fbb",
            "98805d02ce3245c1bf0b751bda9604a3",
            "d37e20b6b6144f34b79fa56c42ec9c86",
            "ed151880b7434edf886aee0a412893a7",
            "828954c115184657b5427501ed883eb3",
            "bf14317840284ee88af5dd1cc671ad35",
            "9e6f23f5c7cc4db2b5d4fb786cb56e1e",
            "f58c817480e3435888c2b70691d5fc3c",
            "c3b82c05dfd343baab2230424cc7ee9c",
            "b84ef85dbb82459587a3e1d612e63a6a",
            "43e78ae59e9041c4b38526d73d05bc17",
            "cd6c978472d94acb90eda4b21dd4a87f",
            "ce04176e0975474187fff2f2a73eb4eb",
            "6adc54197ab4431d9b52b7a8099a53b4",
            "24d650b522ed40078bdc8d64915207f6",
            "57d29470254242c18dd4c15e2b24f8da",
            "441e2eb6143140ee87bdbc68497d7345",
            "ef3afbcbf4d04aa7a260a6944310c4ff",
            "ff4b37dcebfa438da9fa5bc9cfc00b9b",
            "167ed7fb5f0a466ea48b74b8f6683b66",
            "5f1d1de0c0be43dbac3304a90447e852",
            "6f00628a9f3a46319188823e9f22fcc6",
            "43307b1889e847d9a2cf5c1b9111a6ca",
            "734aef2f1af14a219cc67cad10009362",
            "1486239dcffa4323af9593aceb07822f",
            "9916674947844a5faf97d8fa9ca8691e",
            "18ddd0ddd7424bb694e8336d560f0f6b",
            "b46d6dacbbbd455ca2d81e6f2f53a2bc",
            "7ad51e12c7c74b6b81b78cfef8d859b3",
            "cf8b51bc21fd4176bc5c6c53dad0d0d4",
            "75c2a00d6eda4587bc67fe6750e9f254",
            "c1c0a15261d14a04862396f967054529",
            "c7456052d6a348a39fa4d7853bf922a3",
            "e63847d179fa4667bffaee08cde64182",
            "291427c6a3ab4238b0441849f08757e8",
            "e3e9c0ad3bb74687b23c744c7704b6ae",
            "aadbac0f4b1249428fc7173bf10157fc",
            "ceed8a75f11b4c018b62dec8032baeba",
            "68659ba7c8cd4a2484a1841a1529e024",
            "420fe8ec6afc4b85846b84e2ccf547c2",
            "7c41a6d8c5724a64960effbb8aaa74e3",
            "f9839832936248bdbf923e2345736eab",
            "f970c411740940be8b0a15b881dab203",
            "d607792bc47143c08024bd042909457c",
            "bd970302faea4da0b11b216fa1c7b17f",
            "7c35d78693b84d3dac9e32df9dc21d25",
            "6d15a6261fdf4f489fb33bf286d6e0e3",
            "e10691a7cebb48dca7203ea6df0b03a5",
            "13cdb70882f841f6969c6d5ca2238ab1",
            "2729417ed95b49e4a0a13d9cdeb95352",
            "13b7f30e66fb428cb6c474c4c37bcf7f",
            "f16f6af2d6e541b4bf6e18b93cdb82ef",
            "13aa73e1afd9483992753e3c718ce098",
            "2171324886d44c21893b0c791aae5ff4",
            "f9b3b7b139884d2fbaaac1e5f9b74ecb",
            "3531aac1a05547b2946915b8a8245011",
            "6ca9f657cfe04743a531907eaa5c74aa",
            "78b7719f8de94ce595af3fa56d522bc6",
            "0152ef58bebe488e96dafcfb6e67d559",
            "50b14f6bd5264d9f9d7c8d10dac08c87",
            "a546a1047f7f47e9b3f927d6d3c3064e",
            "1b5fdf4e3e0f4eb9ad8c36c1886da303",
            "78a9d98ee127488db26e9e8226bd801c",
            "e600e2344a0c4061ac435e4bc107d39e",
            "eb7ebe5db7a4468dbdf8e5e10b781887",
            "ef1c917762fe4a918939853cf7fe244f",
            "7ff45eca9434468183432e41d9b4848d",
            "61aa032573f24fa5a2a10f8f27d54ee8",
            "373df8c168f14851ae802d25cec34338",
            "210417dd70f840d192f70558d98d53b0",
            "f11b883337f441ea8c16140bcb0924f7",
            "c01e018be6e04b7cb13033ed33b3fa7a",
            "3eb023c1e7994928a1eb699cbbce906a",
            "c2f103b842664a9fbae17503d76cefae",
            "e0c4b97ab05244c3be34cd6b3638cbae",
            "cd918224563142e799dbe8d6eb79b49c",
            "a9a97e6b58394d059ce789fba6a1800a",
            "16cb17d0c88345678f376da4d4976926",
            "908fcd96e6504428aecc59e808bedd3e",
            "4603c4b086ea4c69ae1cb0cfd84118a0",
            "ceb04c28f657437da55e25e02a36ede5",
            "7e2b079222bb412b8a1d539260ed9082",
            "dea52e937dac4a4bb0b17fd2c0889878",
            "7a97732257e3491faf7ba2ccaa496945",
            "ebb232a3ac4f483b886007a4212c94b3",
            "2144bfbd99fe4f10a3123a4a81989a94",
            "249c3b69897c48b38ca757a8e9096852",
            "e8a556e457894766b45b6e893fa77be2",
            "b0a7ce2879544b6b966bb239834f34b9",
            "4cc913965f1f487ba86cdc5f7f0d8d27",
            "42c99dbf1d86457c8a4302024eaad92c",
            "907f99ceee77475ea21e1d6222046b9f",
            "eb0f7a91fbc44772a960d79f3443f849",
            "0a9d5305ed6549b08913d453f2008722",
            "d1fbec7aea1949b8a00d35e516b6ab9e",
            "898b513ee54c4f34a4c2ec19e7b005b7",
            "4a513af041534278b0bd7cc8026fddf0",
            "b61fdf61ed3549ca8d4d7ff52347a0ea",
            "b5fcb7938e6142f89979753e8692804f",
            "78e284572243489f8452563b79f3c6d6",
            "46f756f0732645f0bf35985507d5cb76",
            "35f51b074e314ba8bb31fecbac2a9332",
            "e007ed41492745f79aeb48d4070fc385",
            "50393c02dcd84da48533664ba55c3b2d",
            "34b485be8cb24abf86fa557407d0ddc1",
            "d4093aedc99c4b58b570a1302645f04b",
            "575e65d13e8444e79df14d346ec9f259",
            "9e4219acc0584589a9bb1ae69b2500c9",
            "1d09c66de4f04e5bbf0d29682f7cd6e7",
            "a4fdabcc7a0c4c8593a6cda728c9ed5f",
            "83cec16e8f3741159f1a2ebfc04e409d",
            "1fda37cef35a44949e31436f81ad929c",
            "75459616bd924df48c5848279458f6ad",
            "3faf0805f5864d3bb2fe6e8f004e53be",
            "c63b22cd331e4fbfa854398435c7af12",
            "67e7b9f1194e425d8df1ebbbe6c3b7fb",
            "c47e1d14212347fa9014e426a2eb4511",
            "4656672e1fbc40af9412e59acd211904",
            "ad09e712f0fc4613ba970808d88f5f9a",
            "7f7a7109c173401baa7bbc86e873f395",
            "64e7a432ca624098a8746e8fc462b86b",
            "1c3b336696274f0ab0c0852fb12afc10",
            "c10fcca8129d417ca1e47b170273d6f5",
            "428a89fe706c43a0b5094251933b7df9",
            "54dfebc290994afe9effd3c2d8a8413b",
            "ee506017b2104b4299bdf8fe8ffe39a6",
            "178d7bf0d1204d7cb2b1e89bf5502445",
            "e1af062834084ddba38dd95ab129c9e6",
            "47fe6bbcae7c49b082697217fe6bd50d",
            "0f31b5d070694f558bffba12eeb485e3"
          ]
        },
        "id": "X0EySek8Gigf",
        "outputId": "e27546a3-ccba-498f-d885-3e2bc716116f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c62d27be21a94254b94d86a5e28a191b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "794f58e3ed614e52b6be10682056f4ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02d728ff8c2d47818dc01b32bb90bc9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/437M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96d04647ae0444c4b198c9e2b009d416"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertWithLexicon were not initialized from the model checkpoint at wietsedv/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'lexicon_fc.bias', 'lexicon_fc.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1960s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a752a4f3b61547f9a9a2319829309857"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b84ef85dbb82459587a3e1d612e63a6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f1d1de0c0be43dbac3304a90447e852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [116/116 04:18, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.124200</td>\n",
              "      <td>1.128516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.452400</td>\n",
              "      <td>1.013469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.141300</td>\n",
              "      <td>1.315783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>1.564240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1960s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.33      0.17      0.22        12\n",
            "     neutral       0.17      0.05      0.08        20\n",
            "    positive       0.51      0.82      0.63        33\n",
            "\n",
            "    accuracy                           0.46        65\n",
            "   macro avg       0.34      0.34      0.31        65\n",
            "weighted avg       0.37      0.46      0.38        65\n",
            "\n",
            "Confusion Matrix for 1960s_gas:\n",
            " [[ 2  1  9]\n",
            " [ 2  1 17]\n",
            " [ 2  4 27]]\n",
            "==================================================\n",
            "Processing 1970s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1c0a15261d14a04862396f967054529"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f970c411740940be8b0a15b881dab203"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2171324886d44c21893b0c791aae5ff4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 02:15, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.172436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.008000</td>\n",
              "      <td>1.619533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.210600</td>\n",
              "      <td>1.842796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>2.056671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1970s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.33      0.50         3\n",
            "     neutral       0.00      0.00      0.00         3\n",
            "    positive       0.64      1.00      0.78         9\n",
            "\n",
            "    accuracy                           0.67        15\n",
            "   macro avg       0.55      0.44      0.43        15\n",
            "weighted avg       0.59      0.67      0.57        15\n",
            "\n",
            "Confusion Matrix for 1970s_gas:\n",
            " [[1 0 2]\n",
            " [0 0 3]\n",
            " [0 0 9]]\n",
            "==================================================\n",
            "Processing 1980s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb7ebe5db7a4468dbdf8e5e10b781887"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd918224563142e799dbe8d6eb79b49c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "249c3b69897c48b38ca757a8e9096852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 02:34, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.105900</td>\n",
              "      <td>1.202232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.681200</td>\n",
              "      <td>1.393971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>1.663797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.095300</td>\n",
              "      <td>1.874945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1980s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.67      0.46      0.55        13\n",
            "     neutral       0.33      0.14      0.20         7\n",
            "    positive       0.52      0.85      0.65        13\n",
            "\n",
            "    accuracy                           0.55        33\n",
            "   macro avg       0.51      0.48      0.46        33\n",
            "weighted avg       0.54      0.55      0.51        33\n",
            "\n",
            "Confusion Matrix for 1980s_gas:\n",
            " [[ 6  2  5]\n",
            " [ 1  1  5]\n",
            " [ 2  0 11]]\n",
            "==================================================\n",
            "Processing 1990s_gas...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b61fdf61ed3549ca8d4d7ff52347a0ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d09c66de4f04e5bbf0d29682f7cd6e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f7a7109c173401baa7bbc86e873f395"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 01:33, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.259774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.440427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.697300</td>\n",
              "      <td>1.602747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.697300</td>\n",
              "      <td>1.728579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 1990s_gas:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.43      0.75      0.55         4\n",
            "     neutral       0.00      0.00      0.00         2\n",
            "    positive       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.38         8\n",
            "   macro avg       0.14      0.25      0.18         8\n",
            "weighted avg       0.21      0.38      0.27         8\n",
            "\n",
            "Confusion Matrix for 1990s_gas:\n",
            " [[3 0 1]\n",
            " [2 0 0]\n",
            " [2 0 0]]\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}